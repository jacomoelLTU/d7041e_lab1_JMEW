{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unidecode import unidecode\n",
    "\n",
    "def preprocess_text_file(input_file, output_file, removal, max_rows=1000):\n",
    "    try:\n",
    "        # Read the content of the input file, limited to the first 1000 rows\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read(max_rows)\n",
    "\n",
    "        # Remove characters in the 'removal' set\n",
    "        cleaned_content = ''.join(char for char in content if char not in removal)\n",
    "\n",
    "        # Use unidecode to convert Unicode to ASCII\n",
    "        cleaned_content = unidecode(cleaned_content)\n",
    "\n",
    "        # Write the modified content back to the output file\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(cleaned_content)\n",
    "\n",
    "        #print(f\"Text file '{input_file}' preprocessed and saved to '{output_file}'.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{input_file}' not found.\")\n",
    "\n",
    "# Add escape characters, single quotes, and commas to the removal_characters\n",
    "removal_characters = set(\"0123456789!@#$%^&*()_-+=<>?,./:;{}[]|`~\\\"'\\\\\\t\\n\\r,\")\n",
    "\n",
    "def preprocess_files_in_directory(directory, removal, max_rows=1000):\n",
    "    for subdir, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            input_file_path = os.path.join(subdir, file)\n",
    "            output_file_name = f\"pre-{file}\"\n",
    "            output_file_path = os.path.join(subdir, output_file_name)\n",
    "            preprocess_text_file(input_file_path, output_file_path, removal, max_rows)\n",
    "            #print(\"Pre-processed\", str(input_file_path))\n",
    "\n",
    "# Example usage:\n",
    "#corpus_directory = r'C:\\Users\\wiklu\\OneDrive\\Skrivbord\\LTU\\LTU\\D7041E\\d7041e_labs_JMEW\\lab3\\data\\europarl'\n",
    "corpus_directory = r'C:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\data\\corpus'\n",
    "all_language_dirs = ['bg', 'cs', 'da', 'de', 'el', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n",
    "\n",
    "preprocess_files_in_directory(corpus_directory, removal_characters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing europarl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unidecode import unidecode\n",
    "\n",
    "def preprocess_text_file(input_file, output_file, removal, max_rows=1000):\n",
    "    try:\n",
    "        # Read the content of the input file, limited to the first 1000 rows\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read(max_rows)\n",
    "\n",
    "        # Remove characters in the 'removal' set\n",
    "        cleaned_content = ''.join(char for char in content if char not in removal)\n",
    "\n",
    "        # Use unidecode to convert Unicode to ASCII\n",
    "        cleaned_content = unidecode(cleaned_content)\n",
    "\n",
    "        # Write the modified content back to the output file\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(cleaned_content)\n",
    "        #print(f\"Text file '{input_file}' preprocessed and saved to '{output_file}'.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{input_file}' not found.\")\n",
    "\n",
    "# Add escape characters, single quotes, and commas to the removal_characters\n",
    "removal_characters = set(\"0123456789!@#$%^&*()_-+=<>?,./:;{}[]|`~\\\"'\\\\\\t\\n\\r,\")  # Add other characters you want to remove\n",
    "\n",
    "def preprocess_files_in_directory(directory, removal, max_rows=1000):\n",
    "    for subdir, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            input_file_path = os.path.join(subdir, file)\n",
    "            output_file_name = f\"pre-{os.path.basename(subdir)}_{file}\"\n",
    "            output_file_path = os.path.join(subdir, output_file_name)\n",
    "            preprocess_text_file(input_file_path, output_file_path, removal, max_rows)\n",
    "            #print(\"Pre-processed\", str(input_file_path))\n",
    "\n",
    "# Example usage:\n",
    "#europarl_directory = r'C:\\Users\\wiklu\\OneDrive\\Skrivbord\\LTU\\LTU\\D7041E\\d7041e_labs_JMEW\\lab3\\data\\europarl'\n",
    "europarl_directory = r\"C:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\data\\europarl\"\n",
    "all_language_dirs  = ['bg', 'cs', 'da', 'de', 'el', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n",
    "\n",
    "preprocess_files_in_directory(europarl_directory, removal_characters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove preprocessed files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def remove_preprocessed_files(directory):\n",
    "    for subdir, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.startswith(\"pre-\"):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                os.remove(file_path)\n",
    "                #print(f\"File '{file}' removed.\")\n",
    "\n",
    "# Example usage:\n",
    "#europarl_directory = r'C:\\Users\\wiklu\\OneDrive\\Skrivbord\\LTU\\LTU\\D7041E\\d7041e_labs_JMEW\\lab3\\data\\europarl'\n",
    "europarl_directory = r'r\"C:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\data\\europarl\"'\n",
    "remove_preprocessed_files(europarl_directory)\n",
    "#corpus_directory   = r'C:\\Users\\wiklu\\OneDrive\\Skrivbord\\LTU\\LTU\\D7041E\\d7041e_labs_JMEW\\lab3\\data\\corpus'\n",
    "corpus_directory = r'r\"C:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\data\\europarl\"'\n",
    "remove_preprocessed_files(corpus_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "parent_dir_sentences = r\"C:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\data\\europarl\"\n",
    "own_testing_data = {\"labels\": [], \"sentence\": []}\n",
    "\n",
    "for language_folder in os.listdir(parent_dir_sentences):\n",
    "    language_path = os.path.join(parent_dir_sentences, language_folder)\n",
    "    if os.path.isdir(language_path):\n",
    "        for file_name in os.listdir(language_path):\n",
    "            # Add a condition to check if the file name starts with \"pre-\"\n",
    "            if file_name.startswith(\"pre-\") and os.path.isfile(os.path.join(language_path, file_name)):\n",
    "                file_path = os.path.join(language_path, file_name)\n",
    "                own_testing_data[\"labels\"].append(language_folder)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as my_file_read:\n",
    "                    sentences = my_file_read.read()  # Read the entire content of the file\n",
    "                    # Assuming you want to use the first 100 characters\n",
    "                    own_testing_data[\"sentence\"].append(sentences[:100])\n",
    "\n",
    "#print(own_testing_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "parent_dir_sentences = r\"C:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\data\\europarl\"\n",
    "own_training_data = {\"labels\": [], \"sentence\": []}\n",
    "\n",
    "for language_folder in os.listdir(parent_dir_sentences):\n",
    "    language_path = os.path.join(parent_dir_sentences, language_folder)\n",
    "    if os.path.isdir(language_path):\n",
    "        for file_name in os.listdir(language_path):\n",
    "            # Add a condition to check if the file name starts with \"pre-\"\n",
    "            if file_name.startswith(\"pre-\") and os.path.isfile(os.path.join(language_path, file_name)):\n",
    "                file_path = os.path.join(language_path, file_name)\n",
    "                own_training_data[\"labels\"].append(language_folder)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as my_file_read:\n",
    "                    sentences = my_file_read.read()  # Read the entire content of the file\n",
    "                    # Assuming you want to use the first 100 characters\n",
    "                    own_training_data[\"sentence\"].append(sentences[:100])\n",
    "\n",
    "#print(own_training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "alphabet = unidecode(' abcdefghijklmnopqrstuvwxyz')\n",
    "n = 3\n",
    "N_GRAMS = [p for p in itertools.product(alphabet, repeat=n)]\n",
    "\n",
    "def initialize_random_item_memory(alphabet_size, hd_dimension):\n",
    "    return np.random.choice([-1, 1], size=(alphabet_size, hd_dimension))\n",
    "\n",
    "def consolidate_methods(ngram, hd_memory, permutation_matrix):\n",
    "    ngram_length = len(ngram)\n",
    "    ngram_hd_vector = np.ones(hd_memory.shape[1])\n",
    "\n",
    "    for j in range(ngram_length):\n",
    "        if ngram[j].isspace():\n",
    "            symbol_position = 26\n",
    "        else:\n",
    "            symbol_position = ord(ngram[j]) - ord('a')\n",
    "        permuted_vector = hd_memory[symbol_position] * permutation_matrix[j]\n",
    "        ngram_hd_vector *= permuted_vector\n",
    "\n",
    "    return ngram_hd_vector\n",
    "\n",
    "alphabet_size = 27\n",
    "hdd = 1000\n",
    "ngram = 'cba'\n",
    "\n",
    "random_item_memory = initialize_random_item_memory(alphabet_size, hdd)\n",
    "permutation_matrix = [np.random.choice([-1, 1], size=hdd) for _ in range(n)]\n",
    "\n",
    "encoded_N_GRAMS = {}\n",
    "for i in N_GRAMS:\n",
    "    encoded_N_GRAMS[str(i)] = form_ngram_hd_vector(i, random_item_memory, permutation_matrix)\n",
    "    #print(encoded_N_GRAMS[str(i)])\n",
    "#print(encoded_N_GRAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect n-gram statistics for all training data \n",
    "TR_grams = [[0] * hdd for _ in range(19)]  # Initialize n-gram statistics for training\n",
    "\n",
    "for i in range(len(own_training_data[\"labels\"])):\n",
    "    working_lang = own_training_data[\"sentence\"][i]\n",
    "    #print(own_training_data[\"labels\"][i])\n",
    "\n",
    "    for j in range(len(working_lang) - (n-1)):\n",
    "        ngc = working_lang[j:(j+n)]  # Pick current n-gram\n",
    "        ngc1 = tuple(ngc)\n",
    "\n",
    "        # Increment the corresponding statistics\n",
    "        TR_grams[i] = [a + b for a, b in zip(TR_grams[i], encoded_N_GRAMS.get(str(ngc1), [0] * hdd))]\n",
    "        #print(TR_grams[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect n-gram statistics for all testing data \n",
    "labels_size = len(own_testing_data[\"labels\"])\n",
    "number_of_tests = 5\n",
    "hd_dimension =  1000\n",
    "\n",
    "# Initialize arrays for ground truth and prediction\n",
    "GT = [None] * labels_size * number_of_tests  # ground truth\n",
    "PR = [None] * labels_size * number_of_tests  # prediction\n",
    "\n",
    "# Initialize array for n-gram statistics\n",
    "TS_grams = np.zeros((labels_size * number_of_tests, hd_dimension))\n",
    "\n",
    "# Iterate over each testing example\n",
    "for i in range(labels_size):\n",
    "    working_lang = own_testing_data[\"sentence\"][i]\n",
    "    for j in range(number_of_tests):\n",
    "        try:\n",
    "            working_lang_used = working_lang[j * int(len(working_lang) / number_of_tests):(j + 1) * int(len(working_lang) / number_of_tests)]\n",
    "        except:\n",
    "            working_lang_used = working_lang[j * int(len(working_lang) / number_of_tests):]\n",
    "        # Initialize n-gram statistics for the current test example\n",
    "        for k in range(len(working_lang_used) - (n - 1)):\n",
    "            ngc = working_lang_used[k:(k + n)]  # pick current n-gram\n",
    "            ngc = tuple(ngc)\n",
    "            ngc_stats = encoded_N_GRAMS.get(str(ngc), np.zeros(hd_dimension)) # find index in N_GRAMS\n",
    "            TS_grams[i * number_of_tests + j] += ngc_stats  # increment the corresponding statistics\n",
    "\n",
    "        # Calculate Dot Product and Predicted Language\n",
    "        DP = np.dot(TS_grams[i * number_of_tests + j].reshape(1, -1), np.array(TR_grams).transpose())\n",
    "        ind = np.argmax(DP)  # index of predicted language\n",
    "\n",
    "        # Store Predictions and Ground Truth\n",
    "        PR[i * number_of_tests + j] = own_testing_data[\"labels\"][ind]\n",
    "        GT[i * number_of_tests + j] = own_testing_data[\"labels\"][i] \n",
    "\n",
    "#print(PR)\n",
    "#print(GT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "accuracy=0\n",
    "for i in range(len(GT)):\n",
    "   #print(GT[i])\n",
    "   #print(PR[i])\n",
    "   accuracy+=(PR[i]==GT[i])\n",
    "res_f1 = f1_score(GT, PR, average='weighted')\n",
    "accuracy=accuracy/len(GT) \n",
    "print(f\"Accuracy with {hd_dimension} HD dimension, is {100*accuracy:.2f}%, and f1-Score: {res_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1  Constructing high-dimensional centroids\n",
    "\n",
    "Q : what will be the size of the n-gram input vector in conventional (local) \n",
    "representation? \n",
    "\n",
    "    Firstly the size of the input vector depends on the choice of the n parameter i.e the order of the n-grams since the input vectors size is bounded by the n-grams that can be created from the given alphabet. So for our case where ' abcdefghijklmnopqrstuvwxyz' was the given alphabet (27 chars) and n = 3,  the size of the input vector woule be: 27^3 = 19638 \n",
    "\n",
    "Q :  Identify difficulties of working with conventional representations of n-grams \n",
    "in the machine learning context.\n",
    "\n",
    "Mermory requirments / High dimensionality:\n",
    "\n",
    "    Memory is allways a factor while working with machine learning, we ran into problems with this while performing the lab on one of our members laptop device (an older device). While running the preprocessing i had to limit the amount of text to parse to be able to complete the execution of the functions we created. More generally this problem occurs since storing and processing high-deminsional vectors for large datasets is hardware demanding and thus requier substantial memory resources.\n",
    "\n",
    "    As the size of the input alphabet and the n parameter increase, the dimensions of the input space grows exponentially (for exampel 27^3) which can lead to the curse of dimensionality i.e the dimensions or features increase so much that the data becomes to sparse for the model to make correct predictions. This is a factor contributing to overfitting of the training data and makes it harder for the model to predit the unseen testing data.\n",
    "\n",
    "Lack of continuity: \n",
    "\n",
    "    The continuity or order of words in sequence will not be captured in the local representation since it treats each n-gram independently i.e words sequence will only be captured locally of an offset by the number of n. Thus the sequential order of words in a sentance that carry information could be missed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Classification using hyperdimensional centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_mat= sklearn.metrics.confusion_matrix(GT, PR, labels=None, sample_weight=None)\n",
    "Labels_arr = ['bg', 'cs', 'da', 'de', 'el', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n",
    "#plot confusion matrix\n",
    "%matplotlib inline\n",
    "df_cm = pd.DataFrame(conf_mat, index = [i for i in Labels_arr], columns = [j for j in Labels_arr])\n",
    "plt.figure(figsize = (18,18))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "print(\"Done lab 3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
