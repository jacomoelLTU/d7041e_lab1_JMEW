{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Europarl, the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download all languages Europarl\n",
    "import download\n",
    "import threading\n",
    "\n",
    "def maybe_download_and_extract(language_code=\"sv\"):\n",
    "    url = \"http://www.statmt.org/europarl/v7/\"\n",
    "\n",
    "    # Create the full URL for the file with this data-set.\n",
    "    data_url = url + language_code + \"-en.tgz\"\n",
    "    data_dir = \"data/europarl/\" + language_code + \"/\"\n",
    "\n",
    "    download.download(url=data_url, path=data_dir, kind=\"tar.gz\", progressbar=False, replace=True)\n",
    "\n",
    "\n",
    "def runInParallel():\n",
    "  # We download the files in parallel to speed up and reduce the time needed.\n",
    "  lang_codes = [\"bg\", \"cs\", \"da\", \"de\", \"el\", \"es\", \"et\", \"fi\", \"fr\", \"hu\", \"it\", \"lv\", \"nl\", \"pl\", \"pt\", \"ro\", \"sk\", \"sl\", \"sv\"]\n",
    "\n",
    "  proc = []\n",
    "  for code in lang_codes:\n",
    "    thread = threading.Thread(target=maybe_download_and_extract, args=(code,))\n",
    "    thread.start()\n",
    "    proc.append(thread)\n",
    "  for thread in proc:\n",
    "    thread.join()\n",
    "\n",
    "  print(\"done\")\n",
    "\n",
    "runInParallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Corpora, the train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download all languages Corpora\n",
    "import download\n",
    "import threading\n",
    "\n",
    "def downloading_corpus(url, dir, language_code):\n",
    "    year=2018\n",
    "\n",
    "    data_dir = \"data/\" + dir + \"/\" + language_code + \"/\"\n",
    "    i = 1\n",
    "    while i == 1:\n",
    "        try:\n",
    "            full_url = url + '_newscrawl_' + str(year) + '_10k.tar.gz'\n",
    "            download.download(url=full_url, path=data_dir, kind=\"tar.gz\", progressbar = False, replace=True, verbose=False)\n",
    "            i=0\n",
    "        except:\n",
    "            year = year + 1\n",
    "\n",
    "corpus_lang_codes = [\"bul\", \"ces\", \"dan\", \"deu\", \"ell\", \"eng\", \"est\", \"fin\", \"fra\", \"hun\", \"ita\", \"lav\", \"lit\", \"nld\", \"pol\", \"por\", \"ron\", \"slk\", \"slv\", \"spa\", \"swe\"]\n",
    "\n",
    "proc = []\n",
    "for code in corpus_lang_codes: \n",
    "    url = 'https://downloads.wortschatz-leipzig.de/corpora/' + code\n",
    "    thread = threading.Thread(target=downloading_corpus, args=(url, \"corpus\", code,))\n",
    "    thread.start()\n",
    "    proc.append(thread)\n",
    "\n",
    "for thread in proc:\n",
    "    thread.join()\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nld']\n",
      "[' juist om die ruimte te scheppen op deze site vindt u meer informatie over onze producten in zeeland kan de formatie van naaldwijk direct bovenop de formatie van maassluis liggen op zoek naar een unieke beleving sinds heeft hij een breed scala aan kennis en ervaring opgebouwd in de administratieve informatievoorziening de centrumlinkse kandidaat kreeg iets meer dan procent van de stemmen perfect internetworking solutions stelt de software van realaudio en realvideo gratis beschikbaar we kunnen nu opnieuw uit respect voor de kiezer geen steun verlenen aan heijenk over andere punten die de advocaat van louwes had aangedragen aan de hoge raad adviseert de advocaatgeneraal negatief die zijn in zijn ogen niet voldoende om de zaak ter herzien met de half miljard aan extra middelen moeten docenten worden bijgeschoold en ondersteund ook hierbij is de afsluiting zeer belangrijk om geen reinfectie te krijgen bij sylkamode vindt u een grote kast gevuld met allerlei kralen pijpjeskralen pailletten']\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moell\\AppData\\Local\\Temp\\ipykernel_2140\\3046671508.py:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  chop_size=int(data['chop_size'])\n",
      "C:\\Users\\moell\\AppData\\Local\\Temp\\ipykernel_2140\\3046671508.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  exm_size=int(data['exm_size'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import itertools # for generting all possible n-grams\n",
    "import sklearn.metrics\n",
    "\n",
    "#for visualization\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_g=2 # n-gram size\n",
    "data = scipy.io.loadmat('languages_data.mat')\n",
    "alphabet=(data['alphabet'])\n",
    "alphabet=str(alphabet[0]) # turn into string\n",
    "N_GRAMS=[p for p in itertools.product(alphabet, repeat=n_g)] # get all possible n-grams\n",
    "\n",
    "# get data from the file\n",
    "chop_size=int(data['chop_size'])\n",
    "exm_size=int(data['exm_size'])\n",
    "langLabels=(data['langLabels'])\n",
    "testing=(data['testing'])\n",
    "training=(data['training'])\n",
    "\n",
    "\n",
    "# Print an example of a text in the training data\n",
    "ind=3\n",
    "print(langLabels[0][ind])\n",
    "print(training[0][ind])\n",
    "print (chop_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing 1.1 corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbound method str.isalpha() needs an argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\Languages_ngrams.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m corpus_directory \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mmoell\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mbuild-unitime\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md7041e\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md7041e_lab1_JMEW\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mlab3\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mcorpora\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m corpus_lang_codes \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mbul\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mces\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdan\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdeu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mell\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfra\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhun\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mita\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlav\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnld\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mron\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mslk\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mslv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mspa\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mswe\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m preprocess_and_save_corpus(corpus_directory, corpus_lang_codes)\n",
      "\u001b[1;32mc:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\Languages_ngrams.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     lines \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Preprocessing logic for each line\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m cleaned_lines \u001b[39m=\u001b[39m [preprocess_text(line) \u001b[39mfor\u001b[39;49;00m line \u001b[39min\u001b[39;49;00m lines]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(output_file_path, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     file\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(cleaned_lines))\n",
      "\u001b[1;32mc:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\Languages_ngrams.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     lines \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Preprocessing logic for each line\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m cleaned_lines \u001b[39m=\u001b[39m [preprocess_text(line) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(output_file_path, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     file\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(cleaned_lines))\n",
      "\u001b[1;32mc:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\Languages_ngrams.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_text\u001b[39m(content):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# Replace \".\" with \"\\n\" and ensure consistent line endings\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     cleaned_content \u001b[39m=\u001b[39m content\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39mstr\u001b[39;49m\u001b[39m.\u001b[39;49misalpha(), \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Remove extra whitespaces and normalize line endings\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/moell/build-unitime/d7041e/d7041e_lab1_JMEW/lab3/Languages_ngrams.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     cleaned_content \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(line\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m cleaned_content\u001b[39m.\u001b[39msplitlines())\n",
      "\u001b[1;31mTypeError\u001b[0m: unbound method str.isalpha() needs an argument"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_text(content):\n",
    "    # Replace \".\" with \"\\n\" and ensure consistent line endings\n",
    "    cleaned_content = content.replace('.', '\\n').replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    cleaned_content = ''.join(char if char.isalpha() or char.isspace() else ' ' for char in cleaned_content)\n",
    "\n",
    "    # Remove extra whitespaces and normalize line endings\n",
    "    cleaned_content = ' '.join(line.strip() for line in cleaned_content.splitlines())\n",
    "\n",
    "    return cleaned_content\n",
    "\n",
    "# Function to preprocess and save corpus\n",
    "def preprocess_and_save_corpus(directory, corpus_lang_codes, max_chars=1000):\n",
    "    for lang_code in corpus_lang_codes:\n",
    "        input_file_path = os.path.join(directory, f\"{lang_code}_newscrawl_10K-sentences.txt\")\n",
    "        output_file_path = os.path.join(directory, f\"{lang_code}_newscrawl_10K-sentences_preprocd.txt\")\n",
    "\n",
    "        try:\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # Preprocessing logic for each line\n",
    "            cleaned_lines = [preprocess_text(line) for line in lines]\n",
    "\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write('\\n'.join(cleaned_lines))\n",
    "\n",
    "            print(f\"Preprocessed and saved: {output_file_path}\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{input_file_path}' not found.\")\n",
    "\n",
    "# Example usage\n",
    "corpus_directory = r'C:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\data\\corpora'\n",
    "corpus_lang_codes = [\"bul\", \"ces\", \"dan\", \"deu\", \"ell\", \"eng\", \"est\", \"fin\", \"fra\", \"hun\", \"ita\", \"lav\", \"lit\", \"nld\", \"pol\", \"por\", \"ron\", \"slk\", \"slv\", \"spa\", \"swe\"]\n",
    "\n",
    "preprocess_and_save_corpus(corpus_directory, corpus_lang_codes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing 1.1 europarl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unidecode import unidecode\n",
    "\n",
    "def preprocess_text_file(input_file, output_file, removal, max_rows=1000):\n",
    "    try:\n",
    "        # Read the content of the input file, limited to the first 1000 rows\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read(max_rows)\n",
    "\n",
    "        # Remove characters in the 'removal' array\n",
    "        cleaned_content = ''.join(char for char in content if char not in removal)\n",
    "\n",
    "        # Use unidecode to convert Unicode to ASCII\n",
    "        cleaned_content = unidecode(cleaned_content)\n",
    "\n",
    "        # Write the modified content back to the output file\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(cleaned_content)\n",
    "\n",
    "        print(f\"Text file '{input_file}' preprocessed and saved to '{output_file}'.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{input_file}' not found.\")\n",
    "\n",
    "def preprocess_files_in_directory(directory, removal, max_rows=1000):\n",
    "    for subdir, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            input_file_path = os.path.join(subdir, file)\n",
    "            output_file_name = f\"pre-{os.path.basename(subdir)}_{file}\"\n",
    "            output_file_path = os.path.join(subdir, output_file_name)\n",
    "            preprocess_text_file(input_file_path, output_file_path, removal, max_rows)\n",
    "            print(\"Pre-processed\", str(input_file_path))\n",
    "\n",
    "# Example usage:\n",
    "europarl_directory = r'C:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\data\\europarl'\n",
    "removal_characters = set(\"0123456789!@#$%^&*()_-+=<>?,./:;{}[]|`~\\\"'\\\\\")  # Add other characters you want to remove\n",
    "all_language_dirs  = ['bg', 'cs', 'da', 'de', 'el', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n",
    "\n",
    "preprocess_files_in_directory(europarl_directory, removal_characters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove preprocessed files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def remove_preprocessed_files(directory):\n",
    "    for subdir, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.startswith(\"pre-\"):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                os.remove(file_path)\n",
    "                print(f\"File '{file}' removed.\")\n",
    "\n",
    "# Example usage:\n",
    "europarl_directory = r'C:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\data\\europarl'\n",
    "remove_preprocessed_files(europarl_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "encodingDict = {}\n",
    "\n",
    "for i in letters:\n",
    "    encodingDict[i] = np.random.choice([-1, 1], size=dimensions)\n",
    "\n",
    "print(encodingDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unidecode\n",
    "\n",
    "testS = []\n",
    "s_dict = {}\n",
    "testingLabels = []\n",
    "all_language_dirs  = ['bg', 'cs', 'da', 'de', 'el', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n",
    "\n",
    "letters = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "base_path = r\"C:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\data\\europarl\"\n",
    "\n",
    "for language in all_language_dirs:\n",
    "    folder_path = os.path.join(base_path, language)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(\"pre-\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, mode=\"r\", encoding=\"utf8\") as data:\n",
    "                i = 0\n",
    "                for line in data:\n",
    "                    dt = unidecode.unidecode(line).lower()\n",
    "                    for x in dt:\n",
    "                        if x not in letters:\n",
    "                            dt = dt.replace(x, '')\n",
    "                    ngrams = [dt[i:i + 2] for i in range(len(dt) - 1)]\n",
    "                    for ngram in ngrams:\n",
    "                        if s_dict.get(ngram) is None:\n",
    "                            s_dict[ngram] = 1\n",
    "                        else:\n",
    "                            s_dict[ngram] += 1\n",
    "                testS.append(s_dict)\n",
    "                testingLabels.append(language)\n",
    "                s_dict = {}\n",
    "                i += 1\n",
    "                if i >= 10:\n",
    "                    break\n",
    "\n",
    "print(testS)\n",
    "print(testingLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unidecode\n",
    "\n",
    "valid_characters = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "ngram_frequencies = {}\n",
    "vector_dimensions = 100\n",
    "ngram_length = 3\n",
    "alphabet_size = 27\n",
    "labels = []\n",
    "training_data = []\n",
    "base_path = r\"C:\\Users\\moell\\build-unitime\\d7041e\\d7041e_lab1_JMEW\\lab3\\data\\europarl\"\n",
    "all_language_dirs = ['bg', 'cs', 'da', 'de', 'el', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n",
    "\n",
    "for language in all_language_dirs:\n",
    "    folder_path = os.path.join(base_path, language)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(\"pre-\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, mode=\"r\", encoding=\"utf8\") as file:\n",
    "                name_parts = file_name.split(\"_\", 1)\n",
    "                labels.append(name_parts[0])\n",
    "\n",
    "                for line in file:\n",
    "                    clean_line = unidecode.unidecode(line).lower()\n",
    "\n",
    "                    for char in clean_line:\n",
    "                        if char not in valid_characters:\n",
    "                            clean_line = clean_line.replace(char, '')\n",
    "\n",
    "                    ngrams = [clean_line[i:i + ngram_length] for i in range(len(clean_line) - 1)]\n",
    "\n",
    "                    for ngram in ngrams:\n",
    "                        if ngram not in ngram_frequencies:\n",
    "                            ngram_frequencies[ngram] = 1\n",
    "                        else:\n",
    "                            ngram_frequencies[ngram] += 1\n",
    "\n",
    "                training_data.append(ngram_frequencies)\n",
    "                ngram_frequencies = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def transform_ngram_data(listS):\n",
    "    listH = []\n",
    "    for s in listS:\n",
    "        h = np.zeros(dimensions)       \n",
    "        for ngram in s.keys():\n",
    "            n_i = np.random.choice([1], size = dimensions)\n",
    "            s_i = s[ngram]\n",
    "            j = 1\n",
    "            for x in ngram:\n",
    "                n_i = np.multiply(n_i, np.roll(encodingDict[x], j))\n",
    "                j +=1\n",
    "            h += s_i * n_i\n",
    "        h = h/np.linalg.norm(h)\n",
    "        listH.append(h)\n",
    "    return(listH)\n",
    "\n",
    "training_H = transform_ngram_data(training_data) \n",
    "testing_H = transform_ngram_data(testS)\n",
    "print(training_H)\n",
    "print(testing_H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLabels = []\n",
    "\n",
    "for test in testing_H:\n",
    "    cos = []\n",
    "    \n",
    "    for training in training_H:\n",
    "        cosine = np.dot(test, training) / (np.linalg.norm(test) * np.linalg.norm(training))\n",
    "        cos.append(cosine)\n",
    "    if cos:\n",
    "        maxIndex = np.argmax(cos)\n",
    "        predLabels.append(labels[maxIndex])\n",
    "    else:\n",
    "        # Handle the case where cos is empty (e.g., testing_H or training_H is empty)\n",
    "        predLabels.append(None)\n",
    "\n",
    "print(predLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode_tri_gram(tri_gram):\n",
    "    # Encode each element of the tri-gram to +1 or -1 randomly\n",
    "    encoded_tri_gram = [1 if np.random.rand() > 0.5 else -1 for _ in tri_gram]\n",
    "    return encoded_tri_gram\n",
    "\n",
    "def map_to_hd_vectors(encoded_tri_gram, d1, d2):\n",
    "    # Map the encoded tri-gram to HD vectors\n",
    "    hd_vector_d1 = np.zeros(d1)\n",
    "    hd_vector_d2 = np.zeros(d2)\n",
    "\n",
    "    # Randomly select subsets of coordinates\n",
    "    selected_coordinates_d1 = np.random.choice(d1, size=len(encoded_tri_gram), replace=False)\n",
    "    selected_coordinates_d2 = np.random.choice(d2, size=len(encoded_tri_gram), replace=False)\n",
    "\n",
    "    # Assign values to the selected coordinates\n",
    "    hd_vector_d1[selected_coordinates_d1] = encoded_tri_gram\n",
    "    hd_vector_d2[selected_coordinates_d2] = encoded_tri_gram\n",
    "\n",
    "    return hd_vector_d1, hd_vector_d2\n",
    "\n",
    "def process_text_file(file_path, d1, d2):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    tri_grams = [text[i:i+3] for i in range(len(text)-2)]\n",
    "    \n",
    "    hd_vectors_d1 = []\n",
    "    hd_vectors_d2 = []\n",
    "\n",
    "    for tri_gram in tri_grams:\n",
    "        encoded_tri_gram = encode_tri_gram(tri_gram)\n",
    "        hd_vector_d1, hd_vector_d2 = map_to_hd_vectors(encoded_tri_gram, d1, d2)\n",
    "        hd_vectors_d1.append(hd_vector_d1)\n",
    "        hd_vectors_d2.append(hd_vector_d2)\n",
    "\n",
    "    return np.array(hd_vectors_d1), np.array(hd_vectors_d2)\n",
    "\n",
    "# Example usage\n",
    "file_path = 'your_text_file.txt'  # Replace with the path to your text file\n",
    "d1 = 100\n",
    "d2 = 1000\n",
    "\n",
    "hd_vectors_d1, hd_vectors_d2 = process_text_file(file_path, d1, d2)\n",
    "\n",
    "print(\"HD Vectors (d1):\", hd_vectors_d1)\n",
    "print(\"HD Vectors (d2):\", hd_vectors_d2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect n-gram statitics for all training data \n",
    "TR_grams=np.zeros((len(N_GRAMS),langLabels.size), dtype='float') # initialize n-gram statistics for the  training\n",
    "\n",
    "for i in range(langLabels.size):\n",
    "    print(langLabels[0][i])\n",
    "\n",
    "    buf_chop=training[0][i] #pick the current portion\n",
    "    buf_chop=np.array2string(buf_chop) # get to string    \n",
    "    buf_chop=(buf_chop[3:len(buf_chop)-2]) # get rid of artefacts like []\n",
    "    \n",
    "    for jj in range(len(buf_chop)-(n_g-1)):        \n",
    "        ngc=buf_chop [jj:(jj+(n_g))] #pick current n-gram\n",
    "        ngc1= tuple(ngc)\n",
    "        ind_ngc=N_GRAMS.index(ngc1) # find index in  N_GRAMS\n",
    "        TR_grams[ind_ngc,i]+=1 #increment the corresponding statisticss\n",
    "\n",
    "    TR_grams[:,i]=TR_grams[:,i]/np.linalg.norm(TR_grams[:,i]) # normalize\n",
    "\n",
    "print(TR_grams[:,0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect n-gram statitics for all testing data \n",
    "\n",
    "\n",
    "GT = [None] * langLabels.size * exm_size   #ground truth  \n",
    "PR = [None] * langLabels.size * exm_size   #predicton\n",
    "\n",
    "for i in range(langLabels.size):    \n",
    "    for j in range(exm_size): \n",
    "        TS_gram=np.zeros((len(N_GRAMS),1), dtype='float') # initialize n-gram statistics for the  test example\n",
    "        buf_chop=testing[j][i] #pick the current portion\n",
    "        buf_chop=np.array2string(buf_chop) # get to string    \n",
    "        buf_chop=buf_chop[3:len(buf_chop)-2] # get rid of artefacts like []\n",
    "        \n",
    "        for jj in range(len(buf_chop)-(n_g-1)): \n",
    "            ngc=buf_chop [jj:(jj+(n_g))] #pick current n-gram\n",
    "            ngc= tuple(ngc)\n",
    "            ind_ngc=N_GRAMS.index(ngc) # find index in  N_GRAMS            \n",
    "            TS_gram[ind_ngc,0]+=1 #increment the corresponding statisticss\n",
    "            \n",
    "        DP=np.dot(TS_gram.transpose(),TR_grams)\n",
    "        ind=np.argmax(DP) #  index of predictd language\n",
    "        PR[ i*exm_size +j ]=np.array2string(langLabels[0][ind])\n",
    "        GT[ i*exm_size +j ]=np.array2string(langLabels[0][i]) # add ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy \n",
    "acc=0.0\n",
    "for i in range(len(GT)):\n",
    "   acc+=(PR[i]==GT[i])\n",
    "\n",
    "acc=acc/len(GT) \n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langLabels[0][1][0].tolist()\n",
    "Labels_arr=np.empty(21).astype(str)\n",
    "for i in range(21):\n",
    "    Labels_arr[i]=langLabels[0][i][0]\n",
    "    \n",
    "Labels_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat= sklearn.metrics.confusion_matrix(GT, PR, labels=None, sample_weight=None)\n",
    "\n",
    "#plot confusion matrix\n",
    "%matplotlib inline\n",
    "df_cm = pd.DataFrame(conf_mat, index = [i for i in Labels_arr], columns = [j for j in Labels_arr])\n",
    "plt.figure(figsize = (18,18))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_string = \"Привет, мир!\"  # This means \"Hello, world!\" in Russian\n",
    "\n",
    "print(unicode_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
